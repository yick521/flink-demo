package com.zhugeio.demo.sink;

import com.alibaba.fastjson.JSON;
import com.zhugeio.demo.model.IdOutput;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;
import org.apache.flink.streaming.connectors.kafka.KafkaSerializationSchema;
import org.apache.kafka.clients.producer.ProducerRecord;

import javax.annotation.Nullable;
import java.nio.charset.StandardCharsets;
import java.util.Properties;
import java.util.UUID;

/**
 * Kafka Sinkï¼ˆFlink 1.14å®Œæ•´é€‚é…ç‰ˆ - ä¿®å¤client-idé—®é¢˜ï¼‰
 */
public class KafkaSinkWithCheckpoint {

    /**
     * è‡ªå®šä¹‰Kafkaåºåˆ—åŒ–Schema
     */
    private static class StringKafkaSerializationSchema implements KafkaSerializationSchema<String> {

        private final String topic;

        public StringKafkaSerializationSchema(String topic) {
            this.topic = topic;
        }

        @Override
        public ProducerRecord<byte[], byte[]> serialize(String element, @Nullable Long timestamp) {
            return new ProducerRecord<>(
                    topic,
                    element.getBytes(StandardCharsets.UTF_8)
            );
        }
    }

    /**
     * åˆ›å»ºKafka Producerï¼ˆæ”¯æŒExactly-Onceï¼‰
     * Flink 1.14ç‰ˆæœ¬ - ä¿®å¤client-idé—®é¢˜
     */
    public static FlinkKafkaProducer<String> createKafkaProducer(
            String topic,
            String brokers,
            boolean enableExactlyOnce) {

        Properties kafkaProps = new Properties();
        kafkaProps.setProperty("bootstrap.servers", brokers);

        // ğŸ”§ å…³é”®ä¿®å¤ï¼šè®¾ç½®ç®€çŸ­åˆæ³•çš„client.idï¼ˆé¿å…JMXé”™è¯¯ï¼‰
        // åªä½¿ç”¨å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦
        String clientId = "flink-producer-" + UUID.randomUUID().toString().substring(0, 8);
        kafkaProps.setProperty("client.id", clientId);

        // åˆ›å»ºåºåˆ—åŒ–Schema
        KafkaSerializationSchema<String> serializationSchema =
                new StringKafkaSerializationSchema(topic);

        if (enableExactlyOnce) {
            // Exactly-Onceè¯­ä¹‰é…ç½®
            kafkaProps.setProperty("transaction.timeout.ms", "600000");  // 15åˆ†é’Ÿ

            // è®¾ç½®äº‹åŠ¡IDå‰ç¼€ï¼ˆä¹Ÿè¦ç®€çŸ­åˆæ³•ï¼‰
            kafkaProps.setProperty("transactional.id", "flink-txn-" +
                    UUID.randomUUID().toString().substring(0, 8));

            return new FlinkKafkaProducer<>(
                    topic,                                        // default topic
                    serializationSchema,                          // serialization schema
                    kafkaProps,                                   // producer config
                    FlinkKafkaProducer.Semantic.EXACTLY_ONCE     // exactly-once mode
            );
        } else {
            // At-Least-Onceè¯­ä¹‰é…ç½®
            return new FlinkKafkaProducer<>(
                    topic,
                    serializationSchema,
                    kafkaProps,
                    FlinkKafkaProducer.Semantic.AT_LEAST_ONCE
            );
        }
    }

    /**
     * æ·»åŠ Kafka Sinkåˆ°æ•°æ®æµ
     */
    public static void addKafkaSink(
            DataStream<IdOutput> stream,
            String topic,
            String brokers,
            boolean enableExactlyOnce) {

        // è½¬æ¢ä¸ºJSON
        DataStream<String> jsonStream = stream.map(new MapFunction<IdOutput, String>() {
            @Override
            public String map(IdOutput value) throws Exception {
                return JSON.toJSONString(value);
            }
        }).name("To-JSON-String").uid("to-json-string");

        // æ·»åŠ Kafka Sink
        FlinkKafkaProducer<String> kafkaProducer =
                createKafkaProducer(topic, brokers, enableExactlyOnce);

        jsonStream.addSink(kafkaProducer)
                .name("Kafka-Sink")
                .uid("kafka-sink");
    }
}